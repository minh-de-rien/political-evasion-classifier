{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33989036",
   "metadata": {},
   "source": [
    "# QEvasion – Classical Approach\n",
    "\n",
    "This notebook prepares the QEvasion dataset\n",
    "for the two tasks:\n",
    "\n",
    "- **Task 1 – Clarity-level classification (3-way)**  \n",
    "  `clarity_label` in both `train` and `test`.\n",
    "\n",
    "- **Task 2 – Evasion-level classification (9-way)**  \n",
    "  - In `train`: `evasion_label` is a single gold label per example.  \n",
    "  - In `test`: `evasion_label` is empty on purpose. Instead we have\n",
    "    `annotator1`, `annotator2`, `annotator3`. **Any annotator label is considered correct.**\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Build a unified `text` field = Question + Answer.\n",
    "2. Encode clarity and evasion labels as integers.\n",
    "3. Train TF–IDF + linear models (SVM) for both tasks.\n",
    "4. Implement a special evaluation for Task 2 on the official test set\n",
    "   using the three annotators as a set of acceptable labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a1d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'date', 'president', 'url', 'question_order', 'interview_question', 'interview_answer', 'gpt3.5_summary', 'gpt3.5_prediction', 'question', 'annotator_id', 'annotator1', 'annotator2', 'annotator3', 'inaudible', 'multiple_questions', 'affirmative_questions', 'index', 'clarity_label', 'evasion_label'],\n",
       "        num_rows: 3448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'date', 'president', 'url', 'question_order', 'interview_question', 'interview_answer', 'gpt3.5_summary', 'gpt3.5_prediction', 'question', 'annotator_id', 'annotator1', 'annotator2', 'annotator3', 'inaudible', 'multiple_questions', 'affirmative_questions', 'index', 'clarity_label', 'evasion_label'],\n",
       "        num_rows: 308\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# Load QEvasion from Hugging Face\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13914285",
   "metadata": {},
   "source": [
    "We will work mainly with the `train` and `test` splits and convert them to pandas DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ad885a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>president</th>\n",
       "      <th>interview_question</th>\n",
       "      <th>interview_answer</th>\n",
       "      <th>clarity_label</th>\n",
       "      <th>evasion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>September 10, 2023</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>Q. Of the Biden administration. And accused th...</td>\n",
       "      <td>Well, look, first of all, theI am sincere abou...</td>\n",
       "      <td>Clear Reply</td>\n",
       "      <td>Explicit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>September 10, 2023</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>Q. Of the Biden administration. And accused th...</td>\n",
       "      <td>Well, look, first of all, theI am sincere abou...</td>\n",
       "      <td>Ambivalent</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>September 10, 2023</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>Q. No worries. Do you believe the country's sl...</td>\n",
       "      <td>Look, I think China has a difficult economic p...</td>\n",
       "      <td>Ambivalent</td>\n",
       "      <td>Partial/half-answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>September 10, 2023</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>Q. No worries. Do you believe the country's sl...</td>\n",
       "      <td>Look, I think China has a difficult economic p...</td>\n",
       "      <td>Ambivalent</td>\n",
       "      <td>Dodging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>September 10, 2023</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>Q. I can imagine. It is evening, I'd like to r...</td>\n",
       "      <td>Well, I hope I get to see Mr. Xi sooner than l...</td>\n",
       "      <td>Clear Reply</td>\n",
       "      <td>Explicit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                date  \\\n",
       "0  The President's News Conference in Hanoi, Vietnam  September 10, 2023   \n",
       "1  The President's News Conference in Hanoi, Vietnam  September 10, 2023   \n",
       "2  The President's News Conference in Hanoi, Vietnam  September 10, 2023   \n",
       "3  The President's News Conference in Hanoi, Vietnam  September 10, 2023   \n",
       "4  The President's News Conference in Hanoi, Vietnam  September 10, 2023   \n",
       "\n",
       "         president                                 interview_question  \\\n",
       "0  Joseph R. Biden  Q. Of the Biden administration. And accused th...   \n",
       "1  Joseph R. Biden  Q. Of the Biden administration. And accused th...   \n",
       "2  Joseph R. Biden  Q. No worries. Do you believe the country's sl...   \n",
       "3  Joseph R. Biden  Q. No worries. Do you believe the country's sl...   \n",
       "4  Joseph R. Biden  Q. I can imagine. It is evening, I'd like to r...   \n",
       "\n",
       "                                    interview_answer clarity_label  \\\n",
       "0  Well, look, first of all, theI am sincere abou...   Clear Reply   \n",
       "1  Well, look, first of all, theI am sincere abou...    Ambivalent   \n",
       "2  Look, I think China has a difficult economic p...    Ambivalent   \n",
       "3  Look, I think China has a difficult economic p...    Ambivalent   \n",
       "4  Well, I hope I get to see Mr. Xi sooner than l...   Clear Reply   \n",
       "\n",
       "         evasion_label  \n",
       "0             Explicit  \n",
       "1              General  \n",
       "2  Partial/half-answer  \n",
       "3              Dodging  \n",
       "4             Explicit  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df  = dataset[\"test\"].to_pandas()\n",
    "\n",
    "train_df.head()[[\n",
    "    \"title\",\n",
    "    \"date\",\n",
    "    \"president\",\n",
    "    \"interview_question\",\n",
    "    \"interview_answer\",\n",
    "    \"clarity_label\",\n",
    "    \"evasion_label\",\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abaf23",
   "metadata": {},
   "source": [
    "## 2. Build unified input text\n",
    "\n",
    "For all models, we will use a single text field combining question and answer, for example:\n",
    "\n",
    "> `Question: <question> [SEP] Answer: <answer>`\n",
    "\n",
    "The `[SEP]` is just a separator token in the raw text (for TF–IDF).  \n",
    "For transformers later, we will let the tokenizer handle segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555118f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"clarity_label\"].notna() & train_df[\"clarity_label\"].astype(str).str.strip().ne(\"\")]\n",
    "test_df = test_df[test_df[\"clarity_label\"].notna() & test_df[\"clarity_label\"].astype(str).str.strip().ne(\"\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b335602d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clarity_label</th>\n",
       "      <th>evasion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: Q. Of the Biden administration. And ...</td>\n",
       "      <td>Clear Reply</td>\n",
       "      <td>Explicit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: Q. Of the Biden administration. And ...</td>\n",
       "      <td>Ambivalent</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Q. No worries. Do you believe the co...</td>\n",
       "      <td>Ambivalent</td>\n",
       "      <td>Partial/half-answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: Q. No worries. Do you believe the co...</td>\n",
       "      <td>Ambivalent</td>\n",
       "      <td>Dodging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: Q. I can imagine. It is evening, I'd...</td>\n",
       "      <td>Clear Reply</td>\n",
       "      <td>Explicit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text clarity_label  \\\n",
       "0  Question: Q. Of the Biden administration. And ...   Clear Reply   \n",
       "1  Question: Q. Of the Biden administration. And ...    Ambivalent   \n",
       "2  Question: Q. No worries. Do you believe the co...    Ambivalent   \n",
       "3  Question: Q. No worries. Do you believe the co...    Ambivalent   \n",
       "4  Question: Q. I can imagine. It is evening, I'd...   Clear Reply   \n",
       "\n",
       "         evasion_label  \n",
       "0             Explicit  \n",
       "1              General  \n",
       "2  Partial/half-answer  \n",
       "3              Dodging  \n",
       "4             Explicit  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_text_column(df):\n",
    "    df = df.copy()\n",
    "    q = df[\"interview_question\"].fillna(\"\")\n",
    "    a = df[\"interview_answer\"].fillna(\"\")\n",
    "    df[\"text\"] = \"Question: \" + q + \" [SEP] Answer: \" + a\n",
    "    return df\n",
    "\n",
    "train_df = build_text_column(train_df)\n",
    "test_df  = build_text_column(test_df)\n",
    "\n",
    "train_df[[\"text\", \"clarity_label\", \"evasion_label\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d186f",
   "metadata": {},
   "source": [
    "## 3. Label encoding\n",
    "\n",
    "We map string labels to integer IDs:\n",
    "\n",
    "- **Clarity** (Task 1): 3 labels in both `train` and `test`.\n",
    "- **Evasion** (Task 2): 9 labels in `train`.  \n",
    "  In `test`, `evasion_label` is empty (for Task 2); we will use `annotator1/2/3` there.\n",
    "\n",
    "We build the mapping dictionaries from the **training** data to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbbdd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarity labels: ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ambivalent': 0, 'Clear Non-Reply': 1, 'Clear Reply': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clarity labels: from train and test (just in case)\n",
    "clarity_labels = sorted(\n",
    "    list(set(train_df[\"clarity_label\"].dropna().unique()) |\n",
    "         set(test_df[\"clarity_label\"].dropna().unique()))\n",
    ")\n",
    "\n",
    "print(\"Clarity labels:\", clarity_labels)\n",
    "\n",
    "clarity2id = {lbl: i for i, lbl in enumerate(clarity_labels)}\n",
    "id2clarity = {i: lbl for lbl, i in clarity2id.items()}\n",
    "\n",
    "clarity2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ba655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evasion labels (train): ['Claims ignorance', 'Clarification', 'Declining to answer', 'Deflection', 'Dodging', 'Explicit', 'General', 'Implicit', 'Partial/half-answer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Claims ignorance': 0,\n",
       " 'Clarification': 1,\n",
       " 'Declining to answer': 2,\n",
       " 'Deflection': 3,\n",
       " 'Dodging': 4,\n",
       " 'Explicit': 5,\n",
       " 'General': 6,\n",
       " 'Implicit': 7,\n",
       " 'Partial/half-answer': 8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evasion labels: only from train (test.evasion_label is intentionally empty for Task 2)\n",
    "evasion_labels = sorted(train_df[\"evasion_label\"].dropna().unique())\n",
    "print(\"Evasion labels (train):\", evasion_labels)\n",
    "\n",
    "evasion2id = {lbl: i for i, lbl in enumerate(evasion_labels)}\n",
    "id2evasion = {i: lbl for lbl, i in evasion2id.items()}\n",
    "\n",
    "evasion2id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89cfdf5",
   "metadata": {},
   "source": [
    "Now we add integer columns:\n",
    "\n",
    "- `clarity_id` for both train and test.\n",
    "- `evasion_id` only for rows where `evasion_label` is not empty (in train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08624da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evasion_label</th>\n",
       "      <th>evasion_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Partial/half-answer</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dodging</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Implicit</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deflection</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Implicit</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         evasion_label  evasion_id\n",
       "0             Explicit           5\n",
       "1              General           6\n",
       "2  Partial/half-answer           8\n",
       "3              Dodging           4\n",
       "4             Explicit           5\n",
       "5             Implicit           7\n",
       "6           Deflection           3\n",
       "7             Implicit           7\n",
       "8             Explicit           5\n",
       "9             Explicit           5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add clarity_id everywhere\n",
    "train_df[\"clarity_id\"] = train_df[\"clarity_label\"].map(clarity2id)\n",
    "test_df[\"clarity_id\"]  = test_df[\"clarity_label\"].map(clarity2id)\n",
    "\n",
    "# For evasion, only valid labels in train\n",
    "mask_evasion_valid = train_df[\"evasion_label\"].notna() & (train_df[\"evasion_label\"] != \"\")\n",
    "train_df[\"evasion_id\"] = np.where(\n",
    "    mask_evasion_valid,\n",
    "    train_df[\"evasion_label\"].map(evasion2id),\n",
    "    -1  # -1 = invalid / missing\n",
    ")\n",
    "\n",
    "train_df[[\"evasion_label\", \"evasion_id\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb998c",
   "metadata": {},
   "source": [
    "## 4. Task 1 – Clarity: train / validation / test splits\n",
    "\n",
    "We create:\n",
    "\n",
    "- `clar_train_df` and `clar_val_df` from the original training split, using a stratified split on `clarity_id`.\n",
    "- `clar_test_df` from the original test split (we keep it as the official test set).\n",
    "\n",
    "We stratify by `clarity_id` to keep the proportion of the three clarity classes similar across train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ca6940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3103, 345, 308)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use all training examples for clarity\n",
    "clar_full_train_df = train_df.copy()\n",
    "\n",
    "X_clar_full = clar_full_train_df[\"text\"].values\n",
    "y_clar_full = clar_full_train_df[\"clarity_id\"].values\n",
    "\n",
    "clar_train_idx, clar_val_idx = train_test_split(\n",
    "    np.arange(len(clar_full_train_df)),\n",
    "    test_size=0.1,\n",
    "    stratify=y_clar_full,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clar_train_df = clar_full_train_df.iloc[clar_train_idx].reset_index(drop=True)\n",
    "clar_val_df   = clar_full_train_df.iloc[clar_val_idx].reset_index(drop=True)\n",
    "clar_test_df  = test_df.copy()\n",
    "\n",
    "len(clar_train_df), len(clar_val_df), len(clar_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f4c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train clarity distribution:\n",
      "clarity_label\n",
      "Ambivalent         1836\n",
      "Clear Reply         947\n",
      "Clear Non-Reply     320\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Val clarity distribution:\n",
      "clarity_label\n",
      "Ambivalent         204\n",
      "Clear Reply        105\n",
      "Clear Non-Reply     36\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Test clarity distribution:\n",
      "clarity_label\n",
      "Ambivalent         206\n",
      "Clear Reply         79\n",
      "Clear Non-Reply     23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train clarity distribution:\")\n",
    "print(clar_train_df[\"clarity_label\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Val clarity distribution:\")\n",
    "print(clar_val_df[\"clarity_label\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Test clarity distribution:\")\n",
    "print(clar_test_df[\"clarity_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7e664",
   "metadata": {},
   "source": [
    "## 5. Task 2 – Evasion: train / validation splits (from `train` only)\n",
    "\n",
    "For Task 2, the gold `evasion_label` exists only in the **train** split.\n",
    "We therefore:\n",
    "\n",
    "1. Filter `train_df` to keep only rows with a valid `evasion_label`.\n",
    "2. Split this subset into `ev_train_df` and `ev_val_df` (stratified on `evasion_id`).\n",
    "3. Keep the official `test_df` for later evaluation using `annotator1/2/3`\n",
    "   (where the `evasion_label` column is intentionally empty).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2194cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3103, 345)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only rows with valid evasion labels in train\n",
    "evasion_train_df = train_df[train_df[\"evasion_id\"] != -1].reset_index(drop=True)\n",
    "\n",
    "X_eva_full = evasion_train_df[\"text\"].values\n",
    "y_eva_full = evasion_train_df[\"evasion_id\"].values\n",
    "\n",
    "ev_train_idx, ev_val_idx = train_test_split(\n",
    "    np.arange(len(evasion_train_df)),\n",
    "    test_size=0.1,\n",
    "    stratify=y_eva_full,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ev_train_df = evasion_train_df.iloc[ev_train_idx].reset_index(drop=True)\n",
    "ev_val_df   = evasion_train_df.iloc[ev_val_idx].reset_index(drop=True)\n",
    "\n",
    "len(ev_train_df), len(ev_val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7179ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evasion train distribution:\n",
      "evasion_label\n",
      "Explicit               947\n",
      "Dodging                635\n",
      "Implicit               439\n",
      "General                347\n",
      "Deflection             343\n",
      "Declining to answer    131\n",
      "Claims ignorance       107\n",
      "Clarification           83\n",
      "Partial/half-answer     71\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Evasion val distribution:\n",
      "evasion_label\n",
      "Explicit               105\n",
      "Dodging                 71\n",
      "Implicit                49\n",
      "General                 39\n",
      "Deflection              38\n",
      "Declining to answer     14\n",
      "Claims ignorance        12\n",
      "Clarification            9\n",
      "Partial/half-answer      8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Evasion train distribution:\")\n",
    "print(ev_train_df[\"evasion_label\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Evasion val distribution:\")\n",
    "print(ev_val_df[\"evasion_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bfcfc",
   "metadata": {},
   "source": [
    "## 6. Evaluation helpers\n",
    "\n",
    "We define:\n",
    "- A generic function to print accuracy and macro F1.\n",
    "- Later, a special function for Task 2 test evaluation with multiple annotators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53382f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classification(y_true, y_pred, label_type=\"\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"=== {label_type} ===\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Macro F1 : {macro_f1:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eeab54",
   "metadata": {},
   "source": [
    "## 7. TF–IDF vectorisation\n",
    "\n",
    "We build a TF–IDF representation on the training texts (Task 1 train) \n",
    "and reuse it for both clarity and evasion models as a classical baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c7702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3103, 30000), (345, 30000), (308, 30000))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),    # unigrams + bigrams\n",
    "    max_features=30000,    # can be tuned\n",
    ")\n",
    "\n",
    "# Fit on all clarity train texts (covers most vocabulary)\n",
    "X_tfidf_clar_train = tfidf.fit_transform(clar_train_df[\"text\"].values)\n",
    "X_tfidf_clar_val   = tfidf.transform(clar_val_df[\"text\"].values)\n",
    "X_tfidf_clar_test  = tfidf.transform(clar_test_df[\"text\"].values)\n",
    "\n",
    "X_tfidf_clar_train.shape, X_tfidf_clar_val.shape, X_tfidf_clar_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6016af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3103, 30000), (345, 30000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_eva_train = tfidf.transform(ev_train_df[\"text\"].values)\n",
    "X_tfidf_eva_val   = tfidf.transform(ev_val_df[\"text\"].values)\n",
    "\n",
    "X_tfidf_eva_train.shape, X_tfidf_eva_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0f7d6",
   "metadata": {},
   "source": [
    "## 8. Baseline 1 – Majority class classifier\n",
    "\n",
    "As a very simple baseline, we predict the most frequent class in the training set\n",
    "for each task and evaluate on validation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d796a4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority baseline - clarity (val):\n",
      "=== Clarity (val) ===\n",
      "Accuracy : 0.5913\n",
      "Macro F1 : 0.2477\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74       204\n",
      "           1       0.00      0.00      0.00        36\n",
      "           2       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.59       345\n",
      "   macro avg       0.20      0.33      0.25       345\n",
      "weighted avg       0.35      0.59      0.44       345\n",
      "\n",
      "\n",
      "Majority baseline - clarity (test):\n",
      "=== Clarity (test) ===\n",
      "Accuracy : 0.6688\n",
      "Macro F1 : 0.2672\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80       206\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.67       308\n",
      "   macro avg       0.22      0.33      0.27       308\n",
      "weighted avg       0.45      0.67      0.54       308\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Task 1 – clarity\n",
    "clar_majority = Counter(clar_train_df[\"clarity_id\"]).most_common(1)[0][0]\n",
    "\n",
    "y_clar_val_majority = np.full_like(clar_val_df[\"clarity_id\"].values, clar_majority)\n",
    "y_clar_test_majority = np.full_like(clar_test_df[\"clarity_id\"].values, clar_majority)\n",
    "\n",
    "print(\"Majority baseline - clarity (val):\")\n",
    "eval_classification(clar_val_df[\"clarity_id\"].values, y_clar_val_majority, label_type=\"Clarity (val)\")\n",
    "\n",
    "print(\"\\nMajority baseline - clarity (test):\")\n",
    "eval_classification(clar_test_df[\"clarity_id\"].values, y_clar_test_majority, label_type=\"Clarity (test)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c4b8b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority baseline - evasion (val):\n",
      "=== Evasion (val) ===\n",
      "Accuracy : 0.3043\n",
      "Macro F1 : 0.0519\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00        38\n",
      "           4       0.00      0.00      0.00        71\n",
      "           5       0.30      1.00      0.47       105\n",
      "           6       0.00      0.00      0.00        39\n",
      "           7       0.00      0.00      0.00        49\n",
      "           8       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.30       345\n",
      "   macro avg       0.03      0.11      0.05       345\n",
      "weighted avg       0.09      0.30      0.14       345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\auria\\OneDrive - De Vinci\\TURIN - ITALIE\\LLMs for Software Engineering\\llm_project_T6\\qevasion-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Task 2 – evasion (on train/val only)\n",
    "eva_majority = Counter(ev_train_df[\"evasion_id\"]).most_common(1)[0][0]\n",
    "\n",
    "y_eva_val_majority = np.full_like(ev_val_df[\"evasion_id\"].values, eva_majority)\n",
    "\n",
    "print(\"Majority baseline - evasion (val):\")\n",
    "eval_classification(ev_val_df[\"evasion_id\"].values, y_eva_val_majority, label_type=\"Evasion (val)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d8a5c",
   "metadata": {},
   "source": [
    "## 9. Baseline 2 – TF–IDF + Linear SVM (Task 1: Clarity)\n",
    "\n",
    "We now train a linear SVM (LinearSVC) on TF–IDF features to predict `clarity_id`.\n",
    "This is a strong classical text classification baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58d70fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF–IDF + LinearSVC – clarity (val):\n",
      "=== Clarity (val) ===\n",
      "Accuracy : 0.6493\n",
      "Macro F1 : 0.5667\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74       204\n",
      "           1       0.61      0.39      0.47        36\n",
      "           2       0.56      0.43      0.48       105\n",
      "\n",
      "    accuracy                           0.65       345\n",
      "   macro avg       0.62      0.54      0.57       345\n",
      "weighted avg       0.64      0.65      0.64       345\n",
      "\n",
      "\n",
      "TF–IDF + LinearSVC – clarity (test):\n",
      "=== Clarity (test) ===\n",
      "Accuracy : 0.6266\n",
      "Macro F1 : 0.3938\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.86      0.76       206\n",
      "           1       0.50      0.13      0.21        23\n",
      "           2       0.31      0.16      0.21        79\n",
      "\n",
      "    accuracy                           0.63       308\n",
      "   macro avg       0.50      0.38      0.39       308\n",
      "weighted avg       0.57      0.63      0.58       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "y_clar_train = clar_train_df[\"clarity_id\"].values\n",
    "y_clar_val   = clar_val_df[\"clarity_id\"].values\n",
    "y_clar_test  = clar_test_df[\"clarity_id\"].values\n",
    "\n",
    "clf_clarity = LinearSVC()\n",
    "clf_clarity.fit(X_tfidf_clar_train, y_clar_train)\n",
    "\n",
    "# Predictions\n",
    "y_clar_val_pred  = clf_clarity.predict(X_tfidf_clar_val)\n",
    "y_clar_test_pred = clf_clarity.predict(X_tfidf_clar_test)\n",
    "\n",
    "print(\"TF–IDF + LinearSVC – clarity (val):\")\n",
    "eval_classification(y_clar_val, y_clar_val_pred, label_type=\"Clarity (val)\")\n",
    "\n",
    "print(\"\\nTF–IDF + LinearSVC – clarity (test):\")\n",
    "eval_classification(y_clar_test, y_clar_test_pred, label_type=\"Clarity (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150d928",
   "metadata": {},
   "source": [
    "## 10. Baseline 3 – TF–IDF + Linear SVM (Task 2: Evasion, internal validation)\n",
    "\n",
    "For Task 2, we train another LinearSVC on the subset of `train` with valid `evasion_label`,\n",
    "and evaluate on the internal validation split (`ev_val_df`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b150c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF–IDF + LinearSVC – evasion (internal val):\n",
      "=== Evasion (val, internal) ===\n",
      "Accuracy : 0.3420\n",
      "Macro F1 : 0.2893\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.08      0.12        12\n",
      "           1       0.83      0.56      0.67         9\n",
      "           2       0.80      0.29      0.42        14\n",
      "           3       0.18      0.16      0.17        38\n",
      "           4       0.32      0.35      0.33        71\n",
      "           5       0.41      0.57      0.47       105\n",
      "           6       0.14      0.10      0.12        39\n",
      "           7       0.35      0.27      0.30        49\n",
      "           8       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.34       345\n",
      "   macro avg       0.36      0.26      0.29       345\n",
      "weighted avg       0.34      0.34      0.33       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_eva_train = ev_train_df[\"evasion_id\"].values\n",
    "y_eva_val   = ev_val_df[\"evasion_id\"].values\n",
    "\n",
    "clf_evasion = LinearSVC()\n",
    "clf_evasion.fit(X_tfidf_eva_train, y_eva_train)\n",
    "\n",
    "y_eva_val_pred = clf_evasion.predict(X_tfidf_eva_val)\n",
    "\n",
    "print(\"TF–IDF + LinearSVC – evasion (internal val):\")\n",
    "eval_classification(y_eva_val, y_eva_val_pred, label_type=\"Evasion (val, internal)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf97e01",
   "metadata": {},
   "source": [
    "## 11. Task 2 – Evasion evaluation on test (multiple annotators)\n",
    "\n",
    "On the official `test` split:\n",
    "- `evasion_label` is empty (Task 2).\n",
    "- Instead, `annotator1`, `annotator2`, `annotator3` each provide an evasion label.\n",
    "- According to the dataset description, **any of these annotator labels is considered correct**.\n",
    "\n",
    "We implement an evaluation function that:\n",
    "1. Takes model predictions (one evasion label per example).\n",
    "2. Builds, for each example, the set of acceptable gold labels `G` by collecting the non-empty annotator labels:\n",
    "\n",
    "```text\n",
    "G = {annotator1, annotator2, annotator3} minus {empty}\n",
    "# i.e., keep only labels that are not \"\", None, or NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "611ea317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 308)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_annotator_gold_set(row):\n",
    "    \"\"\"\n",
    "    Build the set of gold evasion labels from annotator1/2/3 for one row.\n",
    "    Empty strings or NaNs are ignored.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for col in [\"annotator1\", \"annotator2\", \"annotator3\"]:\n",
    "        val = row.get(col, None)\n",
    "        if isinstance(val, str) and val != \"\":\n",
    "            labels.append(val)\n",
    "    return set(labels)\n",
    "\n",
    "\n",
    "# Build gold sets for all test examples\n",
    "test_df[\"evasion_gold_set\"] = test_df.apply(get_annotator_gold_set, axis=1)\n",
    "\n",
    "# Filter rows where we have at least one annotator label\n",
    "has_gold = test_df[\"evasion_gold_set\"].apply(lambda s: len(s) > 0)\n",
    "test_eva_df = test_df[has_gold].reset_index(drop=True)\n",
    "\n",
    "len(test_eva_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19ba451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test texts for evasion\n",
    "X_tfidf_eva_test = tfidf.transform(test_eva_df[\"text\"].values)\n",
    "\n",
    "# Predict evasion IDs\n",
    "y_eva_test_pred_ids = clf_evasion.predict(X_tfidf_eva_test)\n",
    "\n",
    "# Convert predictions back to string labels\n",
    "y_eva_test_pred_labels = [id2evasion[i] for i in y_eva_test_pred_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e97f567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.37012987012987014)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute accuracy where a prediction is correct if it matches any annotator label\n",
    "correct_flags = []\n",
    "\n",
    "for pred, gold_set in zip(y_eva_test_pred_labels, test_eva_df[\"evasion_gold_set\"]):\n",
    "    correct_flags.append(pred in gold_set)\n",
    "\n",
    "accuracy_any_annot = np.mean(correct_flags)\n",
    "accuracy_any_annot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cff746",
   "metadata": {},
   "source": [
    "The value above is the **Task 2 test accuracy** under the rule:\n",
    "\n",
    "> A prediction is counted as correct if it matches *any* of the annotators' evasion labels.\n",
    "\n",
    "We can report this alongside the internal validation metrics as a baseline\n",
    "for evasion-level classification on the official test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87d4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qevasion-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
