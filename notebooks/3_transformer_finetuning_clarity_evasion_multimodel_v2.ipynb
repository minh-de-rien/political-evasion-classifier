{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# QEvasion â€“ Multi-Model Transformer Comparison (v2)\n",
                "\n",
                "This notebook extends the **v2** pipeline to test and compare multiple Transformer architectures on the QEvasion dataset.\n",
                "\n",
                "## Objectives\n",
                "1.  **Iterate through 3 Models**:\n",
                "    *   `distilbert-base-uncased` (Baseline)\n",
                "    *   `bert-base-uncased` \n",
                "    *   `albert-base-v2` \n",
                "2.  **Train Multi-Task Models**: Jointly predict Clarity (3-way) and Evasion (9-way).\n",
                "3.  **Visualize Per-Model Performance**: Training curves and Confusion Matrices.\n",
                "4.  **Final Comparison**: Aggregate results and plot a comparative Bar Chart.\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8e7ae2d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# If Colab, run this cell to clone the repo\n",
                "!rm -rf political-evasion-classifier\n",
                "!git clone https://github.com/minh-de-rien/political-evasion-classifier.git\n",
                "%cd political-evasion-classifier\n",
                "import sys\n",
                "sys.path.insert(0, \"/content/political-evasion-classifier\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "124e0bd2",
            "metadata": {
                "id": "setup_imports"
            },
            "outputs": [],
            "source": [
                "# If local\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "project_root = Path(os.getcwd()).parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.append(str(project_root))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a50b196d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Import src modules\n",
                "from src.data import (\n",
                "    load_qevasion_prepared,\n",
                "    prepare_task1_data,\n",
                "    prepare_task2_data,\n",
                "    CLARITY_LABELS, EVASION_LABELS,\n",
                "    CLARITY_TO_ID, EVASION_TO_ID,\n",
                "    ID_TO_CLARITY, ID_TO_EVASION,\n",
                "    build_text_column, add_label_ids, get_annotator_labels\n",
                ")\n",
                "from src.models import MultiTaskTransformer\n",
                "from src.training import train_model, EarlyStopping, evaluate_multitask\n",
                "from src.metrics import evaluate_task2_multi_annotator, plot_confusion_matrix\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7805c499",
            "metadata": {
                "id": "data_prep_md"
            },
            "source": [
                "## 1. Data Preparation\n",
                "We load the dataset once. Tokenization happens inside the training loop as it depends on the specific tokenizer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "9b03a9e2",
            "metadata": {
                "id": "data_load"
            },
            "outputs": [],
            "source": [
                "dataset = load_qevasion_prepared()\n",
                "train_df_raw = dataset[\"train\"].to_pandas()\n",
                "test_df_raw = dataset[\"test\"].to_pandas()\n",
                "\n",
                "# Standard Preprocessing (Text & Labels)\n",
                "# Note: In multi-task, we want rows that have at least a Clarity label.\n",
                "# Evasion labels might be NaN for some (handled by mask).\n",
                "\n",
                "def preprocess_for_multitask(df):\n",
                "    df = build_text_column(df)\n",
                "    df = add_label_ids(df)\n",
                "    return df\n",
                "\n",
                "train_full = preprocess_for_multitask(train_df_raw)\n",
                "test_full = preprocess_for_multitask(test_df_raw)\n",
                "\n",
                "# Add annotator labels for test set (for Evasion Acc calculation)\n",
                "test_full[\"annotator_labels\"] = test_full.apply(get_annotator_labels, axis=1)\n",
                "\n",
                "print(f\"Train size: {len(train_full)}\")\n",
                "print(f\"Test size:  {len(test_full)}\")\n",
                "\n",
                "# Visualize Label Distribution (as requested)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "clarity_counts = train_full['clarity_label'].value_counts()\n",
                "sns.barplot(x=clarity_counts.index, y=clarity_counts.values, ax=axes[0], palette='viridis')\n",
                "axes[0].set_title('Clarity Label Distribution')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "evasion_counts = train_full['evasion_label'].value_counts()\n",
                "sns.barplot(x=evasion_counts.index, y=evasion_counts.values, ax=axes[1], palette='magma')\n",
                "axes[1].set_title('Evasion Label Distribution')\n",
                "axes[1].tick_params(axis='x', rotation=45, labelsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "292f1c3b",
            "metadata": {
                "id": "dataset_class"
            },
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset\n",
                "\n",
                "class MultiTaskDataset(Dataset):\n",
                "    def __init__(self, df, tokenizer, max_length=256):\n",
                "        self.df = df.reset_index(drop=True)\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        text = row['text']\n",
                "        \n",
                "        enc = self.tokenizer(\n",
                "            text, \n",
                "            max_length=self.max_length, \n",
                "            padding='max_length', \n",
                "            truncation=True, \n",
                "            return_tensors='pt'\n",
                "        )\n",
                "\n",
                "        item = {\n",
                "            'input_ids': enc['input_ids'].squeeze(0),\n",
                "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
                "            'clarity_labels': torch.tensor(row['clarity_id'], dtype=torch.long)\n",
                "        }\n",
                "\n",
                "        # Handle Evasion (might be -1 if missing)\n",
                "        ev_id = row['evasion_id']\n",
                "        if ev_id != -1:\n",
                "            item['evasion_labels'] = torch.tensor(ev_id, dtype=torch.long)\n",
                "            item['evasion_mask'] = torch.tensor(1, dtype=torch.long)\n",
                "        else:\n",
                "            item['evasion_labels'] = torch.tensor(-1, dtype=torch.long)\n",
                "            item['evasion_mask'] = torch.tensor(0, dtype=torch.long)\n",
                "            \n",
                "        return item"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "acab0589",
            "metadata": {
                "id": "exp_config"
            },
            "source": [
                "## 2. Experiment Configuration\n",
                "We define the list of models to loop over."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "008ca9fb",
            "metadata": {
                "id": "models_list"
            },
            "outputs": [],
            "source": [
                "MODELS_TO_TEST = [\n",
                "    \"distilbert-base-uncased\",\n",
                "    \"bert-base-uncased\",\n",
                "    \"albert-base-v2\"\n",
                "]\n",
                "\n",
                "BATCH_SIZE = 16\n",
                "MAX_LENGTH = 256\n",
                "NUM_EPOCHS = 5\n",
                "LEARNING_RATE = 2e-5\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Create Train/Val split (stratified by Clarity for consistency)\n",
                "train_indices, val_indices = train_test_split(\n",
                "    np.arange(len(train_full)),\n",
                "    test_size=0.1,\n",
                "    stratify=train_full[\"clarity_id\"].values,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "df_train = train_full.iloc[train_indices]\n",
                "df_val = train_full.iloc[val_indices]\n",
                "df_test = test_full\n",
                "\n",
                "print(f\"Train split: {len(df_train)}\")\n",
                "print(f\"Val split:   {len(df_val)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "28d8509e",
            "metadata": {
                "id": "main_loop_md"
            },
            "source": [
                "## 3. Training & Evaluation Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff0b81d1",
            "metadata": {
                "id": "main_loop_code"
            },
            "outputs": [],
            "source": [
                "all_results = []\n",
                "\n",
                "for model_name in MODELS_TO_TEST:\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(f\"MODEL: {model_name}\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # 1. Tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    \n",
                "    # 2. DataLoaders\n",
                "    train_ds = MultiTaskDataset(df_train, tokenizer, MAX_LENGTH)\n",
                "    val_ds   = MultiTaskDataset(df_val, tokenizer, MAX_LENGTH)\n",
                "    test_ds  = MultiTaskDataset(df_test, tokenizer, MAX_LENGTH)\n",
                "    \n",
                "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
                "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
                "    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
                "    \n",
                "    # 3. Model Setup\n",
                "    model = MultiTaskTransformer(\n",
                "        model_name=model_name,\n",
                "        num_clarity_labels=len(CLARITY_TO_ID),\n",
                "        num_evasion_labels=len(EVASION_TO_ID)\n",
                "    ).to(device)\n",
                "    \n",
                "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    early_stopping = EarlyStopping(patience=2, mode='max', verbose=True)\n",
                "    \n",
                "    # 4. Training\n",
                "    print(f\"Training {model_name}...\")\n",
                "    history = train_model(\n",
                "        model=model,\n",
                "        train_loader=train_loader,\n",
                "        val_loader=val_loader,\n",
                "        optimizer=optimizer,\n",
                "        scheduler=None,\n",
                "        clarity_loss_fn=criterion,\n",
                "        evasion_loss_fn=criterion,\n",
                "        device=device,\n",
                "        num_epochs=NUM_EPOCHS,\n",
                "        early_stopping=early_stopping,\n",
                "        is_multitask=True,\n",
                "        verbose=True\n",
                "    )\n",
                "\n",
                "    # 5. Plot Training History\n",
                "    plt.figure(figsize=(12, 4))\n",
                "    \n",
                "    # Loss\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(history['train_loss'], label='Train Loss')\n",
                "    plt.title(f'{model_name}: Loss over Epochs')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.ylabel('Loss')\n",
                "    plt.legend()\n",
                "    \n",
                "    # F1 (Clarity)\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.plot(history['val_macro_f1'], label='Val Clarity F1', color='orange')\n",
                "    plt.title(f'{model_name}: Validation Clarity F1')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.ylabel('Macro F1')\n",
                "    plt.legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    # 6. Final Evaluation on Test\n",
                "    # Get Standard metrics\n",
                "    test_metrics = evaluate_multitask(model, test_loader, device)\n",
                "    \n",
                "    # Get Predictions for Multi-Annotator Evasion Acc\n",
                "    model.eval()\n",
                "    all_ev_preds = []\n",
                "    with torch.no_grad():\n",
                "        for batch in test_loader:\n",
                "            input_ids = batch[\"input_ids\"].to(device)\n",
                "            mask = batch[\"attention_mask\"].to(device)\n",
                "            _, ev_logits = model(input_ids, mask)\n",
                "            preds = torch.argmax(ev_logits, dim=-1)\n",
                "            all_ev_preds.extend(preds.cpu().numpy())\n",
                "    \n",
                "    pred_labels_str = [ID_TO_EVASION[i] for i in all_ev_preds]\n",
                "    gold_sets = df_test[\"annotator_labels\"].tolist()\n",
                "    \n",
                "    # Filter to only rows that have annotators (should be all in test_df filter)\n",
                "    correct_count = 0\n",
                "    total_count = 0\n",
                "    for pred, gold in zip(pred_labels_str, gold_sets):\n",
                "        if len(gold) > 0:\n",
                "            if pred in gold:\n",
                "                correct_count += 1\n",
                "            total_count += 1\n",
                "    \n",
                "    ev_multi_acc = correct_count / total_count if total_count > 0 else 0.0\n",
                "    \n",
                "    # Store Results (Updated with detailed metrics)\n",
                "    res = {\n",
                "        \"Model\": model_name,\n",
                "        \"Clarity_Acc\": test_metrics.get(\"clarity_accuracy\", 0),\n",
                "        \"Clarity_F1\": test_metrics.get(\"clarity_macro_f1\", 0),\n",
                "        \"Evasion_Acc_Std\": test_metrics.get(\"evasion_accuracy\", 0),\n",
                "        \"Evasion_F1_Std\": test_metrics.get(\"evasion_macro_f1\", 0),\n",
                "        \"Evasion_Acc_Multi\": ev_multi_acc,\n",
                "        \"Total_Score\": (test_metrics.get(\"clarity_macro_f1\", 0) + ev_multi_acc) / 2\n",
                "    }\n",
                "    all_results.append(res)\n",
                "    \n",
                "    print(f\"Results for {model_name}:\")\n",
                "    print(f\"  Clarity Acc: {res['Clarity_Acc']:.4f}, F1: {res['Clarity_F1']:.4f}\")\n",
                "    print(f\"  Evasion Acc (Std): {res['Evasion_Acc_Std']:.4f}, F1 (Std): {res['Evasion_F1_Std']:.4f}\")\n",
                "    print(f\"  Evasion Acc (Multi): {res['Evasion_Acc_Multi']:.4f}\")\n",
                "    print(f\"  Total Score: {res['Total_Score']:.4f}\")\n",
                "    \n",
                "    # 7. Confusion Matrix (Clarity)\n",
                "    # We need predictions again for Confusion Matrix\n",
                "    all_cl_preds = []\n",
                "    all_cl_labels = []\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        for batch in test_loader:\n",
                "            input_ids = batch[\"input_ids\"].to(device)\n",
                "            mask = batch[\"attention_mask\"].to(device)\n",
                "            cl_labels = batch[\"clarity_labels\"]\n",
                "            cl_logits, _ = model(input_ids, mask)\n",
                "            preds = torch.argmax(cl_logits, dim=-1)\n",
                "            all_cl_preds.extend(preds.cpu().numpy())\n",
                "            all_cl_labels.extend(cl_labels.cpu().numpy())\n",
                "            \n",
                "    plot_confusion_matrix(\n",
                "        np.array(all_cl_labels),\n",
                "        np.array(all_cl_preds),\n",
                "        label_names=CLARITY_LABELS,\n",
                "        title=f\"{model_name}: Clarity Confusion Matrix\"\n",
                "    )\n",
                "    \n",
                "    # Free memory\n",
                "    del model, optimizer, train_loader, val_loader\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2cbb9149",
            "metadata": {
                "id": "final_results_md"
            },
            "source": [
                "## 4. Final Comparison\n",
                "Aggregating results across all tested models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7230840",
            "metadata": {
                "id": "results_viz"
            },
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(all_results)\n",
                "print(\"\\n=== FINAL SUMMARY ===\")\n",
                "display(results_df)\n",
                "\n",
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 4, figsize=(20, 6))\n",
                "\n",
                "# Clarity F1 Comparison\n",
                "sns.barplot(data=results_df, x=\"Model\", y=\"Clarity_F1\", ax=axes[0], palette=\"Blues_d\")\n",
                "axes[0].set_title(\"Clarity F1\", fontsize=12)\n",
                "axes[0].set_ylim(0, 1.0)\n",
                "for i, v in enumerate(results_df[\"Clarity_F1\"]):\n",
                "    axes[0].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
                "\n",
                "# Clarity Accuracy Comparison\n",
                "sns.barplot(data=results_df, x=\"Model\", y=\"Clarity_Acc\", ax=axes[1], palette=\"Purples_d\")\n",
                "axes[1].set_title(\"Clarity Accuracy\", fontsize=12)\n",
                "axes[1].set_ylim(0, 1.0)\n",
                "for i, v in enumerate(results_df[\"Clarity_Acc\"]):\n",
                "    axes[1].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
                "\n",
                "# Evasion Accuracy Comparison (Multi)\n",
                "sns.barplot(data=results_df, x=\"Model\", y=\"Evasion_Acc_Multi\", ax=axes[2], palette=\"Greens_d\")\n",
                "axes[2].set_title(\"Evasion Acc (Multi)\", fontsize=12)\n",
                "axes[2].set_ylim(0, 1.0)\n",
                "for i, v in enumerate(results_df[\"Evasion_Acc_Multi\"]):\n",
                "    axes[2].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
                "\n",
                "# Total Score Comparison\n",
                "sns.barplot(data=results_df, x=\"Model\", y=\"Total_Score\", ax=axes[3], palette=\"Reds_d\")\n",
                "axes[3].set_title(\"Total Score (Avg)\", fontsize=12)\n",
                "axes[3].set_ylim(0, 1.0)\n",
                "for i, v in enumerate(results_df[\"Total_Score\"]):\n",
                "    axes[3].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_to_csv"
            },
            "outputs": [],
            "source": [
                "results_df.to_csv(\"multimodel_v2_results.csv\", index=False)\n",
                "print(\"Results saved to multimodel_v2_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
