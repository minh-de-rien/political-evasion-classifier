\section{Background}
\label{sec:background}

Evasion and equivocation have long been studied in political communication, where speakers may avoid directly answering questions through ambiguity, reframing, or topic shifts while maintaining rhetorical control \cite{RASIAH2010664}. Such behaviors are generally considered intentional and strategically motivated, providing a basis for distinguishing between replies, ambiguous responses, and non-replies.

In natural language processing, political evasion detection has recently been formalized as a supervised learning problem. The CLARITY shared task builds on the QEvasion dataset, which introduces a hierarchical taxonomy of response clarity and fine-grained evasion strategies, along with baseline transformer-based models \cite{thomas2024isaidthatdataset}. This formulation emphasizes the relationship between questions and answers rather than analyzing responses in isolation.

Related work on answer relevance and quality assessment has shown that surface-level similarity is often insufficient to determine whether a response addresses a question, motivating joint question--answer modeling approaches \cite{Alvarez_2025, farea2022evaluationquestionansweringsystems}. Additionally, response clarity and evasion classification involve subjective judgments, and annotator disagreement often reflects genuine interpretive variation rather than noise \cite{joseph2017constancemodelingannotationcontexts}. These observations motivate evaluation protocols that account for annotation variability.
