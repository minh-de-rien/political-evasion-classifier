Question evasion is a recurrent phenomenon in political communication, where speakers respond to questions without directly addressing the requested information. The CLARITY shared task at SemEval-2026 addresses this challenge by framing political evasion detection as supervised classification over political question--answer pairs, with subtasks focusing on response clarity and evasion strategy identification.

In this work, we conduct a systematic study of modeling approaches for political evasion detection on the QEvasion dataset. We compare classical machine learning baselines with transformer-based models that jointly encode questions and answers, and we evaluate both single-task and multi-task learning settings. In addition, we explore a prompt-based approach using a large language model to perform clarity and evasion classification through in-context learning. This study highlights the strengths and limitations of different modeling paradigms for capturing pragmatic aspects of political responses.
