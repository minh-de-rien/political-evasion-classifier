\section{System Overview}
\label{sec:system_overview}

This section describes the complete system design and methodological choices adopted in our study. We detail the dataset formulation, baseline and neural models, training procedures, hyperparameter tuning strategy, evaluation protocol, and code architecture.

\subsection{Dataset and Task Formulation}
\label{subsec:data-repr}

We use the QEvasion dataset~\cite{qevasion}, which consists of political question--answer pairs annotated for two related classification tasks. The dataset contains 3,448 training examples and 308 test examples. We create a validation set by randomly splitting 10\% of the training data (approximately 345 examples) with stratification to preserve label distributions.

\paragraph{Task 1: Clarity Classification.}
Given a question--answer pair, the goal is to predict whether the answer is a \textit{Clear Reply}, a \textit{Clear Non-Reply}, or \textit{Ambivalent} (3-way classification). This task is fully annotated for all examples and provides a stable supervision signal.

\paragraph{Task 2: Evasion Strategy Classification.}
For answers annotated with an evasion label, the objective is to identify the specific evasion strategy among nine categories: \textit{Explicit}, \textit{Dodging}, \textit{Implicit}, \textit{General}, \textit{Deflection}, \textit{Declining to answer}, \textit{Claims ignorance}, \textit{Clarification}, and \textit{Partial half-answer}. This task exhibits severe class imbalance (Table~\ref{tab:class-dist}), and the official test set provides multiple annotator labels per example.

\paragraph{Input Representation.}
All models receive the concatenated question and answer text as input. For transformer models, we format the sequence as:
\[
\texttt{[CLS] question [SEP] answer [SEP]}
\]
This representation encourages the model to learn the semantic relation between the question and the answer rather than relying only on answer text.

\subsection{Baseline Models: N-grams and TF--IDF}
\label{subsec:baselines}

To establish reference performance levels, we implement classical machine learning baselines based on TF--IDF-weighted n-grams.

\paragraph{Text Representation.}
Each question--answer pair is represented using TF--IDF features extracted from unigrams and bigrams. Given a term $t$ and a document $d$, the TF--IDF weight is defined as:
\[
\text{tfidf}(t, d) = \text{tf}(t, d) \cdot \log \frac{N}{\text{df}(t)}
\]
where $\text{tf}(t, d)$ is the term frequency of $t$ in $d$, $\text{df}(t)$ is the number of documents containing $t$, and $N$ is the total number of documents.

We restrict the vocabulary to 30,000 features, apply minimum document frequency ($\text{min\_df}=2$), and remove very frequent terms ($\text{max\_df}=0.95$) to reduce noise and improve generalization.

\paragraph{Classical Classifiers.}
On top of TF--IDF features, we train:
\begin{itemize}
    \item \textbf{Majority Class:} always predicts the most frequent class,
    \item \textbf{Logistic Regression:} trained with balanced class weights,
    \item \textbf{Linear SVM:} one-vs-rest linear SVM with balanced class weights.
\end{itemize}
These baselines quantify how far surface lexical cues alone can go for evasion detection.

\subsection{Transformer-Based Models}
\label{subsec:transformers}

To capture deeper semantic and contextual information, we fine-tune multiple pretrained transformer encoders. Rather than relying on a single architecture, we evaluate several backbones to study the impact of model size and design while using a consistent fine-tuning protocol.

\paragraph{Backbones.}
We experiment with:
\begin{itemize}
    \item \textbf{DistilBERT-base-uncased}~\cite{sanh2019distilbert}: 6 layers, 66M parameters,
    \item \textbf{BERT-base-uncased}: 12 layers, 110M parameters,
    \item \textbf{ALBERT-base-v2}: parameter-efficient architecture with cross-layer parameter sharing.
\end{itemize}

\subsubsection{Single-Task Models}

In the single-task setting, separate models are trained for Task~1 and Task~2 by placing a classification head on top of the \texttt{[CLS]} representation.

\paragraph{Clarity Model (Task 1).}
We use standard cross-entropy loss:
\[
\mathcal{L}_{\text{clarity}} = -\frac{1}{N}\sum_{i=1}^N \log p_\theta(y_i \mid x_i)
\]

\paragraph{Evasion Model (Task 2).}
Task~2 is highly imbalanced (Table~\ref{tab:class-dist}), so we adopt Focal Loss~\cite{lin2017focal}:
\[
\mathcal{L}_{\text{evasion}} = -\frac{1}{N}\sum_{i=1}^{N} (1 - p_{t,i})^{\gamma} \log(p_{t,i})
\]
where $p_{t,i}$ is the predicted probability for the true class. We experiment with $\gamma \in \{0,1,2,3\}$ and select $\gamma = 2.0$ based on validation performance.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Evasion Strategy} & \textbf{Train} & \textbf{\%} \\
\midrule
Explicit & 1052 & 30.5\% \\
Dodging & 706 & 20.5\% \\
Implicit & 488 & 14.2\% \\
General & 386 & 11.2\% \\
Deflection & 381 & 11.0\% \\
Declining to answer & 145 & 4.2\% \\
Claims ignorance & 119 & 3.5\% \\
Clarification & 92 & 2.7\% \\
Partial half-answer & 79 & 2.3\% \\
\bottomrule
\end{tabular}
\caption{Class distribution for Task 2 (Evasion) in the training set, highlighting severe imbalance (14.4$\times$ between the most and least frequent classes).}
\label{tab:class-dist}
\end{table}

\subsubsection{Multi-Task Learning}
\label{subsubsec:multitask}

We also implement a multi-task architecture that jointly predicts clarity (3-way) and evasion strategy (9-way). The encoder is shared and two task-specific heads are trained on top of the shared \texttt{[CLS]} representation (Figure~\ref{fig:multitask-arch}).

\paragraph{Architecture.}
Each head consists of a dropout layer ($p=0.1$) followed by a linear projection to the corresponding number of classes.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
    \begin{tikzpicture}[
        node distance=0.8cm,
        box/.style={rectangle, draw, minimum width=3.5cm, minimum height=0.7cm, align=center},
        arrow/.style={->, >=stealth, thick}
    ]
        \node[box] (input) {\texttt{[CLS] question [SEP] answer [SEP]}};
        \node[box, below=of input, fill=blue!10] (encoder) {Transformer Encoder \\ (shared backbone)};
        \node[box, below=of encoder, fill=green!10] (cls) {\texttt{[CLS]} representation \\ $h \in \mathbb{R}^{768}$};
        \node[box, below left=0.8cm and 1.5cm of cls, fill=orange!10] (head1) {Clarity Head \\ (3-way)};
        \node[box, below right=0.8cm and 1.5cm of cls, fill=purple!10] (head2) {Evasion Head \\ (9-way)};
        \node[box, below=of head1] (out1) {$p(\text{clarity} \mid x)$};
        \node[box, below=of head2] (out2) {$p(\text{evasion} \mid x)$};

        \draw[arrow] (input) -- (encoder);
        \draw[arrow] (encoder) -- (cls);
        \draw[arrow] (cls) -- (head1);
        \draw[arrow] (cls) -- (head2);
        \draw[arrow] (head1) -- (out1);
        \draw[arrow] (head2) -- (out2);
    \end{tikzpicture}
}
\caption{Multi-task architecture with a shared transformer encoder and two task-specific classification heads.}
\label{fig:multitask-arch}
\end{figure}

\paragraph{Joint Training Objective.}
We optimize the sum of task losses:
\[
\mathcal{L}_{\text{total}} = \lambda_{\text{clarity}}\mathcal{L}_{\text{clarity}} + \lambda_{\text{evasion}}\mathcal{L}_{\text{evasion}}
\]
with $\lambda_{\text{clarity}}=\lambda_{\text{evasion}}=1.0$ in our experiments.



\subsection{Training Procedure and Hyperparameter Tuning}
\label{subsec:training}

All transformer models are fine-tuned using AdamW with weight decay. To stabilize optimization, we employ a linear learning-rate schedule with warmup: the learning rate increases during the first 10\% of training steps and then decreases linearly for the remainder of training. We train for a maximum of 5 epochs and apply early stopping based on validation macro F1-score (patience = 3), retaining the best-performing checkpoint.

Hyperparameters that affect optimization dynamics (notably learning rate and weight decay) are tuned on Task~1 using the DistilBERT backbone. Task~1 provides a fully labeled and stable validation signal compared to Task~2, which exhibits stronger imbalance and higher annotation variability. Once selected, the same hyperparameter configuration is reused for all backbones (DistilBERT, BERT, ALBERT), Task~2, and multi-task experiments to ensure a fair comparison.

\subsection{Evaluation Protocol}
\label{subsec:evaluation}

\paragraph{Task 1 (Clarity).}
We report accuracy, macro F1, weighted F1, precision, and recall on validation and test sets. Macro F1 is used as the primary metric due to class imbalance.

\paragraph{Task 2 (Evasion).}
For training and validation, each example has a single evasion label and we use standard single-label metrics. For the official test set, multiple annotator labels may be provided for a single example. Following the dataset design, we use a relaxed multi-annotator evaluation: a prediction is considered correct if it matches \emph{any} annotator label. Formally, for prediction $\hat{y}_i$ and gold label set $G_i$:
\[
\text{Correct}(i) = \mathbb{I}[\hat{y}_i \in G_i]
\]

\subsection{LLaMA / LLM-based Approach}
\label{subsec:llama}
This part is currently under development and will be included in the final version (missing notebook).

\subsection{Implementation Details}

The system is implemented in Python using PyTorch and the Hugging Face Transformers library~\cite{wolf2019transformers}. The project is structured into modular components:
\begin{itemize}
    \item \texttt{data.py}: dataset loading, preprocessing, label encoding, and split construction,
    \item \texttt{models.py}: model definitions (single-task and multi-task) and custom loss functions (Focal Loss),
    \item \texttt{training.py}: training/evaluation loops, early stopping, scheduler integration, and multi-task masking logic,
    \item \texttt{metrics.py}: computation of classification metrics and multi-annotator evaluation for Task~2.
\end{itemize}

Experiments are conducted on Google Colab using a T4 GPU with 16GB of VRAM.
