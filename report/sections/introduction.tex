\section{Introduction}
In public political discourse, answering a question does not necessarily mean providing information. Politicians frequently respond in ways that appear relevant while sidestepping the core intent of the question, a strategy that can influence audience interpretation without making explicit commitments. Such evasive behavior is particularly prevalent in high-visibility settings such as televised interviews and debates, where communicative choices are often deliberate and strategic.

Understanding and identifying question evasion is important for the analysis of political communication, media accountability, and public trust. However, automatically detecting evasive responses remains challenging, as it requires more than surface-level semantic similarity; effective systems must reason about intent, relevance, and pragmatic alignment between questions and answers.

The CLARITY shared task at SemEval-2026 addresses this challenge by framing equivocation detection as a classification problem over political questionâ€“answer pairs. The task provides annotated data drawn from real-world political interactions and encourages the development of models capable of capturing nuanced forms of evasion.

In this work, we investigate whether modern pre-trained language models can effectively identify evasive responses when explicitly modeling the relationship between a question and its answer. Rather than focusing solely on answer content, our approach emphasizes joint encoding as a means to capture mismatches between question intent and response substance. The remainder of this paper details our system design, experimental setup, results, and a discussion of observed limitations.


\textit{Describe here the objectives and the context of application of your experiment. This text can be based on the application description or on the SEMeval task requirements
}

\subsection{Research Questions} \textit{list the research questions of your experiment.}
